'use server';

import OpenAI from 'openai';
import axios from 'axios';
import * as fs from 'fs';
import path from 'path';
import {createClient} from '@deepgram/sdk';

import {
	AiImageAsset,
	Asset,
	AssetState,
	StockImageAsset,
	StockVideoAsset,
	VoiceoverAsset,
} from '../revideo/types';

const deepgram = createClient(process.env['DEEPGRAM_API_KEY'] || '');

const openai = new OpenAI({
	apiKey: process.env.OPENAI_API_KEY,
});

// { "objects": [ { "text": "Hello World", "fontSize": 50, "fontType": "Roboto", "startTime": 0, "endTime": 3, "appearInAnimation": { "type": "fade", "duration": 1 }, "disappearAnimation": { "type": "fade", "duration": 1 } } ] }
function buildInstruction(instruction: string, sceneState: any, assetState: any) {
	const SYSTEM = `
You are an AI copilot embedded into a video editor. Users can chat with you to create and edit videos, and your job is to create a JSON video representation that is converted into an actual video. You can also create assets, which are based on files, through function calls. The video format is 1920x1080.

Here is the typescript scene definition:

\`\`\`
export interface SceneDefinition {
	objects: SceneObject[];
}

export type SceneObject =
	| TextObject
	| ShapeObject
	| ImageObject
	| VideoObject
	| AudioObject
	| SubtitleObject;

export interface BaseSceneObject {
	type: 'text' | 'rect' | 'image' | 'video' | 'audio' | 'subtitle';
	startTime: number; // at which second the object appears
	endTime: number; // at which second the object disappears, has to be higher than startTime
	position: {x: number; y: number}; // 0,0 is center of video. If you have a 1920x1080 video, the top right corner is {x: 960, y: -540}.
	color: string; // hex code, or also just "white", "blue", etc.
	appearInAnimation?: AppearAnimation; // the animation that makes the object appear, e.g. a fade-in
	disappearAnimation?: AppearAnimation; // the animation that makes the object disappear, e.g. a fade-out
	animations?: Animation[]; // general animations, like scaling up or moving objects to specific coordinates
}

export interface AppearAnimation {
	type: 'fade' | 'scale';
	duration: number;
}

export type Length = number | \`\${number}%\`;

export interface TextObject extends BaseSceneObject {
	type: 'text';
	fontSize: number; // from 10 to 200
	fontFamily: 'Roboto' | 'Luckiest Guy';
	textContent: string; // the actual text
}

export interface ShapeObject extends BaseSceneObject {
	type: 'rect';
	width: Length;
	height: Length;
}

export interface ImageObject extends BaseSceneObject {
	type: 'image';
	src: string;
	width?: Length; // Optional width and height for the image. You may want to specify only one of them to maintain the aspect ratio.
	height?: Length;
}

export interface VideoObject extends BaseSceneObject {
	type: 'video';
	src: string;
	videoStartTime?: number; // At which second the video should start playing. Defaults to 0.
	height?: Length; // Optional height for the video. You may want to specify only one of them to maintain the aspect ratio.
	width?: Length;
}

export interface AudioObject extends BaseSceneObject {
	type: 'audio';
	src: string;
	audioStartTime?: number; // At which second the audio should start playing. Defaults to 0.
}

export interface SubtitleObject {
	type: 'subtitle';
	startTime: number;
	words: VoiceoverAsset['properties']['words'];
}

export interface Animation {
	type: 'moveTo' | 'changeOpacity' | 'scale';
	options: MoveToAnimationOptions | OpacityAnimationOptions | ScaleAnimationOptions;
	startTime: number;
	endTime: number;
}

export interface MoveToAnimationOptions {
	x: number; // for x and y, we only indicate the diff with respect to the current position, not the absolute position, so this is relative to the actual position value
	y: number;
}

export interface OpacityAnimationOptions {
	targetOpacity: number;
}

export interface ScaleAnimationOptions {
	targetScale: number; // refers to the factor the size is scaled by, e.g. two means double of the original size
}

// Assets can be generated by instructions from the AI assistant

export interface AssetState {
	assets: Asset[];
}

export type Asset = VoiceoverAsset | AiImageAsset | StockImageAsset | StockVideoAsset;

export interface VoiceoverAsset {
	type: 'voiceover';
	instructions: {
		text: string;
		voice: 'Sarah' | 'Michael';
	};
	properties: {
		filePath: string; // path to the audio file of the voiceover
		words: WordInfo[]; // word timestamps
	};
}

export interface AiImageAsset {
	type: 'ai_image';
	instructions: {
		prompt: string;
	};
	properties?: {
		filePath: string; // path to the audio file of the voiceover
	};
}
export interface StockImageAsset {
	type: 'stock_image';
	instructions: {
		prompt: string;
		orientation: 'horizontal' | 'vertical';
	};
	properties?: {
		filePath: string;
	};
}

export interface StockVideoAsset {
	type: 'stock_video';
	instructions: {
		prompt: string;
	};
	properties?: {
		filePath: string;
		duration: number;
		width: number;
		height: number;
	};
}

export interface WordInfo {
	word: string;
	start: number; // start time in seconds
	end: number;
}

\`\`\`


Here is the initial state of the scene:

\`\`\`
${JSON.stringify(sceneState)}
\`\`\`

Here is a definition of all assets you can create:

\`\`\`
// Assets can be generated by instructions from the AI assistant

export interface AssetState {
	assets: Asset[];
}

export type Asset = VoiceoverAsset | AiImageAsset | StockImageAsset | StockVideoAsset;

export interface VoiceoverAsset {
	type: 'voiceover';
	instructions: {
		text: string;
		voice: 'Sarah' | 'Michael';
	};
	properties?: {
		filePath: string; // path to the audio file of the voiceover
		words: WordInfo[]; // word timestamps
	};
}

export interface AiImageAsset {
	type: 'ai_image';
	instructions: {
		prompt: string;
	};
	properties?: {
		filePath: string; // path to the audio file of the voiceover
	};
}
export interface StockImageAsset {
	type: 'stock_image';
	instructions: {
		prompt: string;
		orientation: 'horizontal' | 'vertical';
	};
	properties?: {
		filePath: string;
	}
}

export interface StockVideoAsset {
	type: 'stock_video';
	instructions: {
		prompt: string;
	};
	properties?: {
		filePath: string;
		duration: number;
		width: number;
		height: number;
	}
}

export interface WordInfo {
	word: string;
	start: number; // start time in seconds
	end: number;
}
\`\`\`


And here are the assets available to you:

\`\`\`
${JSON.stringify(assetState)}
\`\`\`

A user will provide an instruction. You should only return a JSON, and that JSON should be one of the following formats:

1. If all assets you need are available, you should create a scene description (that might make use of the assets) of the type sceneDefinition. 
2. If you first need to generate assets, you should only return a list that you need to generate currently as type AssetState, and not including the already generated assets. Every asset should have the instruction object along with it, and then functions will be called according to the instruction properties that you gave. Once the assets are generated, you will be prompted again along with the new generated assets to provide a scene object, but this happens in a seperate step.

Importantly, the JSON representation of the scene or the assets should follow the type definition! Here is the user message:

${instruction}`;

	return SYSTEM;
}

export async function sendInstructionToGPT(
	instruction: string,
	sceneState: any,
	assets: AssetState,
): Promise<{sceneState: any; assetState: any}> {
	try {
		console.log('sending instruction', buildInstruction(instruction, sceneState, assets));
		const chatCompletion = await openai.chat.completions.create({
			model: 'o1-preview',
			messages: [{role: 'user', content: buildInstruction(instruction, sceneState, assets)}],
		});

		console.log('response', chatCompletion.choices[0].message.content);

		if (!chatCompletion.choices[0].message.content) {
			throw Error('empty response');
		}

		const jsonString = findOutermostJSON(chatCompletion.choices[0].message.content);
		if (jsonString) {
			const jsonResponse = JSON.parse(jsonString);

			if (jsonResponse.objects) {
				return {sceneState: jsonResponse, assetState: assets};
			}

			if (jsonResponse.assets) {
				const newAssets = await handleAssetGeneration(jsonResponse.assets);
				const updatedAssets = {assets: [...assets.assets, ...newAssets]};
				const response = await sendInstructionToGPT(instruction, sceneState, updatedAssets);
				return response;
			}

			throw new Error('error in gpt response');
		} else {
			throw new Error('No valid JSON found in response');
		}
	} catch (error) {
		console.error('Error in sendInstructionToGPT:', error);
		throw new Error('Failed to process the instruction');
	}
}

async function handleAssetGeneration(assets: Asset[]): Promise<Asset[]> {
	const generatedAssets = [];
	for (const a of assets) {
		if (a.type === 'ai_image') {
			const asset = await generateImage(a.instructions);
			generatedAssets.push(asset);
		}
		if (a.type === 'voiceover') {
			const asset = await voiceover(a.instructions);
			generatedAssets.push(asset);
		}
		if (a.type === 'stock_image') {
			const asset = await downloadPixabayImage(a.instructions);
			generatedAssets.push(asset);
		}
		if (a.type === 'stock_video') {
			const asset = await downloadPixabayVideo(a.instructions);
			generatedAssets.push(asset);
		}
	}

	return generatedAssets;
}

async function generateImage({prompt}: AiImageAsset['instructions']): Promise<AiImageAsset> {
	const jobId = crypto.randomUUID();
	const filePath = `${jobId}-image.png`;

	await dalleGenerate(prompt, `./public/${filePath}`);

	return {
		type: 'ai_image',
		instructions: {prompt},
		properties: {filePath},
	};
}

export async function voiceover({
	text,
	voice,
}: VoiceoverAsset['instructions']): Promise<VoiceoverAsset> {
	const jobId = crypto.randomUUID();
	const filePath = `${jobId}-audio.wav`;
	await generateAudio(text, voice, `./public/${filePath}`);
	const unprocessedWords = await getWordTimestamps(`./public/${filePath}`);
	const words = processWordTimestamps(unprocessedWords);

	console.log('words', words);

	return {
		type: 'voiceover',
		instructions: {text, voice: voice as any},
		properties: {filePath, words},
	};
}

// Function to find the outermost JSON object
function findOutermostJSON(str: string): string | null {
	let depth = 0;
	let start = -1;

	for (let i = 0; i < str.length; i++) {
		if (str[i] === '{') {
			if (depth === 0) start = i;
			depth++;
		} else if (str[i] === '}') {
			depth--;
			if (depth === 0 && start !== -1) {
				return str.substring(start, i + 1);
			}
		}
	}

	return null;
}

function processWordTimestamps(words: any[]): any[] {
	return words.map((word) => ({
		word: word.punctuated_word,
		start: Number(word.start.toFixed(2)),
		end: Number(word.end.toFixed(2)),
	}));
}

export async function generateAudio(text: string, voiceName: string, savePath: string) {
	const data = {
		model_id: 'eleven_multilingual_v2',
		text: text,
	};

	const voiceId = await getVoiceByName(voiceName);

	const response = await axios.post(
		`https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`,
		data,
		{
			headers: {
				'Content-Type': 'application/json',
				'xi-api-key': process.env.ELEVEN_API_KEY || '',
			},
			responseType: 'arraybuffer',
		},
	);

	fs.writeFileSync(savePath, response.data);
}

async function getVoiceByName(name: string) {
	const response = await fetch('https://api.elevenlabs.io/v1/voices', {
		method: 'GET',
		headers: {
			'xi-api-key': process.env.ELEVEN_API_KEY || '',
		},
	});

	if (!response.ok) {
		throw new Error(`HTTP error! status: ${response.status}`);
	}

	const data: any = await response.json();
	const voice = data.voices.find((voice: {name: string; voice_id: string}) => voice.name === name);
	return voice ? voice.voice_id : null;
}

export async function getWordTimestamps(audioFilePath: string) {
	const {result} = await deepgram.listen.prerecorded.transcribeFile(
		fs.readFileSync(audioFilePath),
		{
			model: 'nova-2',
			smart_format: true,
		},
	);

	if (result) {
		return result.results.channels[0].alternatives[0].words;
	} else {
		throw Error('transcription result is null');
	}
}

export async function dalleGenerate(prompt: string, savePath: string) {
	const response = await openai.images.generate({
		model: 'dall-e-3',
		prompt: prompt,
		size: '1792x1024',
		quality: 'standard',
		n: 1,
	});

	if (!response.data || !response.data[0]) {
		throw new Error('No image generated');
	}

	const url = response.data[0].url;
	const responseImage = await axios.get(url || '', {
		responseType: 'arraybuffer',
	});
	const buffer = Buffer.from(responseImage.data, 'binary');

	try {
		await fs.promises.writeFile(savePath, buffer);
	} catch (error) {
		console.error('Error saving the file:', error);
		throw error; // Rethrow the error so it can be handled by the caller
	}
}

export async function downloadPixabayImage({
	prompt,
	orientation,
}: StockImageAsset['instructions']): Promise<StockImageAsset> {
	const apiKey = process.env.PIXABAY_API_KEY || '46433167-3479f6d15735811fd862c2ab8';
	const baseUrl = 'https://pixabay.com/api/';

	const params = new URLSearchParams({
		key: apiKey,
		q: encodeURIComponent(prompt),
		...{orientation},
	});

	try {
		const response = await axios.get(`${baseUrl}?${params.toString()}`);
		const data = response.data;

		if (data.hits && data.hits.length > 0) {
			const image = data.hits[0];
			const imageUrl = image.largeImageURL;
			const imageResponse = await axios.get(imageUrl, {responseType: 'arraybuffer'});
			const buffer = Buffer.from(imageResponse.data, 'binary');

			const fileName = `pixabay-${image.id}.jpg`;
			const filePath = path.join('./public', fileName);
			await fs.promises.writeFile(filePath, buffer);

			return {
				type: 'stock_image',
				instructions: {
					prompt,
					orientation,
				},
				properties: {
					filePath: fileName,
				},
			};
		} else {
			throw new Error('No images found for the given query');
		}
	} catch (error) {
		console.error('Error downloading Pixabay image:', error);
		throw error;
	}
}

export async function downloadPixabayVideo({prompt}: {prompt: string}): Promise<StockVideoAsset> {
	const apiKey = process.env.PIXABAY_API_KEY || '46433167-3479f6d15735811fd862c2ab8';
	const baseUrl = 'https://pixabay.com/api/videos/';

	const params = new URLSearchParams({
		key: apiKey,
		q: encodeURIComponent(prompt),
	});

	try {
		const response = await axios.get(`${baseUrl}?${params.toString()}`);
		const data = response.data;

		if (data.hits && data.hits.length > 0) {
			const video = data.hits[0];
			const videoUrl = video.videos.large.url; // Using the large version
			const videoResponse = await axios.get(videoUrl, {responseType: 'arraybuffer'});
			const buffer = Buffer.from(videoResponse.data, 'binary');

			const fileName = `pixabay-video-${video.id}.mp4`;
			const filePath = path.join('./public', fileName);
			await fs.promises.writeFile(filePath, buffer);

			return {
				type: 'stock_video',
				instructions: {
					prompt,
				},
				properties: {
					filePath: fileName,
					duration: video.duration,
					width: video.videos.large.width,
					height: video.videos.large.height,
				},
			};
		} else {
			throw new Error('No videos found for the given query');
		}
	} catch (error) {
		console.error('Error downloading Pixabay video:', error);
		throw error;
	}
}
