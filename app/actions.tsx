'use server';

import OpenAI from 'openai';
import axios from 'axios';
import * as fs from 'fs';
import {createClient} from '@deepgram/sdk';

import {AiImageAsset, Asset, AssetState, VoiceoverAsset} from '../revideo/scene';

const deepgram = createClient(process.env['DEEPGRAM_API_KEY'] || '');

const openai = new OpenAI({
	apiKey: process.env.OPENAI_API_KEY,
});

// { "objects": [ { "text": "Hello World", "fontSize": 50, "fontType": "Roboto", "startTime": 0, "endTime": 3, "appearInAnimation": { "type": "fade", "duration": 1 }, "disappearAnimation": { "type": "fade", "duration": 1 } } ] }
function buildInstruction(instruction: string, sceneState: any, assetState: any) {
	const SYSTEM = `
You are an AI copilot embedded into a video editor. Users can chat with you to create and edit videos, and your job is to create a JSON video representation that is converted into an actual video. You can also create assets, which are based on files, through function calls. The video format is 1920x1080.

Here is the typescript scene definition:

\`\`\`
export interface SceneDefinition {
	objects: SceneObject[];
}

type SceneObject = TextObject | ShapeObject | ImageObject | VideoObject;

export interface BaseSceneObject {
	type: 'text' | 'rect' | 'image' | 'video';
	startTime: number; // at which second the object appears
	endTime: number; // at which second the object disappears, has to be higher than startTime
	position: {x: number; y: number}; // 0,0 is center of video. If you have a 1920x1080 video, the top right corner is {x: 960, y: -540}.
	color: string; // hex code, or also just "white", "blue", etc.
	appearInAnimation?: AppearAnimation; // the animation that makes the object appear, e.g. a fade-in
	disappearAnimation?: AppearAnimation; // the animation that makes the object disappear, e.g. a fade-out
	animations?: Animation[]; // general animations, like scaling up or moving objects to specific coordinates
}

interface AppearAnimation {
	type: 'fade' | 'scale';
	duration: number;
}

export type Length = number | \`\${number}%\`;

export interface TextObject extends BaseSceneObject {
	type: 'text';
	fontSize: number; // from 10 to 200
	fontFamily: 'Roboto' | 'Luckiest Guy';
	textContent: string; // the actual text
}

interface ShapeObject extends BaseSceneObject {
	type: 'rect';
	width: Length;
	height: Length;
}

interface ImageObject extends BaseSceneObject {
	type: 'image';
	src: string;
	width?: Length; // Optional width and height for the image. You may want to specify only one of them to maintain the aspect ratio.
	height?: Length;
}

interface VideoObject extends BaseSceneObject {
	type: 'video';
	src: string;
	videoStartTime?: number; // At which second the video should start playing. Defaults to 0.
	duration: number; // For how long the video should play from the startTime in seconds.
	height?: Length; // Optional height for the video. You may want to specify only one of them to maintain the aspect ratio.
	width?: Length;
}

interface Animation {
	type: 'moveTo' | 'changeOpacity' | 'scale';
	options: MoveToAnimationOptions | OpacityAnimationOptions | ScaleAnimationOptions;
	startTime: number;
	endTime: number;
}

interface MoveToAnimationOptions {
	x: number; // for x and y, we only indicate the diff with respect to the current position, not the absolute position, so this is relative to the actual position value
	y: number;
}

interface OpacityAnimationOptions {
	targetOpacity: number;
}

interface ScaleAnimationOptions {
	targetScale: number; // refers to the factor the size is scaled by, e.g. two means double of the original size
}

// Assets can be generated by instructions from the AI assistant

export interface AssetState {
	assets: Asset[];
}

export interface Asset {
	type: 'voiceover' | 'ai_image';
	asset: VoiceoverAsset | AiImageAsset;
}

export interface VoiceoverAsset {
	instructions: {
		text: string;
		voice: 'Sarah' | 'Michael';
	};
	properties?: {
		filePath: string; // path to the audio file of the voiceover
		words: WordInfo[]; // word timestamps
	};
}

export interface AiImageAsset {
	instructions: {
		prompt: string;
	};
	properties?: {
		filePath: string; // path to the audio file of the voiceover
	};
}

export interface WordInfo {
	word: string;
	start: number; // start time in seconds
	end: number;
}
\`\`\`


Here is the initial state of the scene:

\`\`\`
${JSON.stringify(sceneState)}
\`\`\`

Here is a definition of all assets you can create:

\`\`\`
// Assets can be generated by instructions from the AI assistant

export interface AssetState {
    assets: Asset[];
}

export interface Asset {
    type: "voiceover" | "ai_image";
    asset: VoiceoverAsset | AiImageAsset;
}

export interface VoiceoverAsset {
    instructions: {
        text: string;
        voice: "Sarah" | "Michael";
    }
    properties?: {
        filePath: string; // path to the audio file of the voiceover
        words: WordInfo[] // word timestamps
    }
}

export interface AiImageAsset {
    instructions: {
        prompt: string;
    }
    properties?: {
        filePath: string; // path to the audio file of the voiceover
        words: WordInfo[] // word timestamps
    }
}

export interface WordInfo {
    word: string;
    start: number; // start time in seconds
    end: number;
}
\`\`\`


And here are the assets available to you:

\`\`\`
${JSON.stringify(assetState)}
\`\`\`

A user will provide an instruction. You should only return a JSON, and that JSON should be one of the following formats:

1. If all assets you need are available, you should create a scene description (that might make use of the assets) of the type sceneDefinition. 
2. If you first need to generate assets, you should only return a list that you need to generate currently as type AssetState, and not including the already generated assets. Every asset should have the instruction object along with it, and then functions will be called according to the instruction properties that you gave. Once the assets are generated, you will be prompted again along with the new generated assets to provide a scene object, but this happens in a seperate step.

Importantly, the JSON representation of the scene or the assets should follow the type definition! Here is the user message:

${instruction}`;

	return SYSTEM;
}

export async function sendInstructionToGPT(
	instruction: string,
	sceneState: any,
	assets: AssetState,
): Promise<{sceneState: any; assetState: any}> {
	try {
		console.log('sending instruction', buildInstruction(instruction, sceneState, assets));
		const chatCompletion = await openai.chat.completions.create({
			model: 'o1-preview',
			messages: [{role: 'user', content: buildInstruction(instruction, sceneState, assets)}],
		});

		console.log('response', chatCompletion.choices[0].message.content);

		if (!chatCompletion.choices[0].message.content) {
			throw Error('empty response');
		}

		const jsonString = findOutermostJSON(chatCompletion.choices[0].message.content);
		if (jsonString) {
			const jsonResponse = JSON.parse(jsonString);

			if (jsonResponse.objects) {
				return {sceneState: jsonResponse, assetState: assets};
			}

			if (jsonResponse.assets) {
				const assets = await handleAssetGeneration(jsonResponse.assets);
				const response = sendInstructionToGPT(instruction, sceneState, {assets: assets});
				return response;
			}

			throw new Error('error in gpt response');
		} else {
			throw new Error('No valid JSON found in response');
		}
	} catch (error) {
		console.error('Error in sendInstructionToGPT:', error);
		throw new Error('Failed to process the instruction');
	}
}

async function handleAssetGeneration(assets: Asset[]): Promise<Asset[]> {
	const generatedAssets = [];
	for (const a of assets) {
		if (a.type === 'ai_image') {
			const aiImageAsset = a.asset as AiImageAsset;
			const asset = await generateImage(aiImageAsset.instructions);
			generatedAssets.push(asset);
		}
		if (a.type === 'voiceover') {
			const voiceoverAsset = a.asset as VoiceoverAsset;
			const asset = await voiceover(voiceoverAsset.instructions);
			generatedAssets.push(asset);
		}
	}

	return generatedAssets;
}

async function generateImage({
	prompt,
}: AiImageAsset['instructions']): Promise<{type: 'ai_image'; asset: AiImageAsset}> {
	const jobId = crypto.randomUUID();
	const filePath = `./public/${jobId}-image.png`;

	return {
		type: 'ai_image',
		asset: {
			instructions: {prompt},
			properties: {filePath},
		},
	};
}

// Function to find the outermost JSON object
function findOutermostJSON(str: string): string | null {
	let depth = 0;
	let start = -1;

	for (let i = 0; i < str.length; i++) {
		if (str[i] === '{') {
			if (depth === 0) start = i;
			depth++;
		} else if (str[i] === '}') {
			depth--;
			if (depth === 0 && start !== -1) {
				return str.substring(start, i + 1);
			}
		}
	}

	return null;
}

export async function voiceover({
	text,
	voice,
}: VoiceoverAsset['instructions']): Promise<{type: 'voiceover'; asset: VoiceoverAsset}> {
	const jobId = crypto.randomUUID();
	const filePath = `./public/${jobId}-audio.wav`;
	await generateAudio(text, voice, filePath);
	const unprocessedWords = await getWordTimestamps(filePath);
	const words = processWordTimestamps(unprocessedWords);

	console.log('words', words);

	return {
		type: 'voiceover',
		asset: {
			instructions: {text, voice: voice as any},
			properties: {filePath, words},
		},
	};
}

function processWordTimestamps(words: any[]): any[] {
	return words.map((word) => ({
		word: word.punctuated_word,
		start: Number(word.start.toFixed(2)),
		end: Number(word.end.toFixed(2)),
	}));
}

export async function generateAudio(text: string, voiceName: string, savePath: string) {
	const data = {
		model_id: 'eleven_multilingual_v2',
		text: text,
	};

	const voiceId = await getVoiceByName(voiceName);

	const response = await axios.post(
		`https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`,
		data,
		{
			headers: {
				'Content-Type': 'application/json',
				'xi-api-key': process.env.ELEVEN_API_KEY || '',
			},
			responseType: 'arraybuffer',
		},
	);

	fs.writeFileSync(savePath, response.data);
}

async function getVoiceByName(name: string) {
	const response = await fetch('https://api.elevenlabs.io/v1/voices', {
		method: 'GET',
		headers: {
			'xi-api-key': process.env.ELEVEN_API_KEY || '',
		},
	});

	if (!response.ok) {
		throw new Error(`HTTP error! status: ${response.status}`);
	}

	const data: any = await response.json();
	const voice = data.voices.find((voice: {name: string; voice_id: string}) => voice.name === name);
	return voice ? voice.voice_id : null;
}

export async function getWordTimestamps(audioFilePath: string) {
	const {result} = await deepgram.listen.prerecorded.transcribeFile(
		fs.readFileSync(audioFilePath),
		{
			model: 'nova-2',
			smart_format: true,
		},
	);

	if (result) {
		return result.results.channels[0].alternatives[0].words;
	} else {
		throw Error('transcription result is null');
	}
}

export async function dalleGenerate(prompt: string, savePath: string) {
	const response = await openai.images.generate({
		model: 'dall-e-3',
		prompt: prompt,
		size: '1792x1024',
		quality: 'standard',
		n: 1,
	});

	if (!response.data || !response.data[0]) {
		throw new Error('No image generated');
	}

	const url = response.data[0].url;
	const responseImage = await axios.get(url || '', {
		responseType: 'arraybuffer',
	});
	const buffer = Buffer.from(responseImage.data, 'binary');

	try {
		await fs.promises.writeFile(savePath, buffer);
	} catch (error) {
		console.error('Error saving the file:', error);
		throw error; // Rethrow the error so it can be handled by the caller
	}
}
